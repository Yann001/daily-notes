# 数学之美

> 吴军 著  第二版



## 第1章 文字和语言 VS 数字和信息

讲述文字、数字和语言的历史。

提到的概念和主题：

- 通信的原理和信息传播的模型
- （信源）编码和最短编码
- 解码的规则，语法
- 聚类
- 校验位
- 双语对照文本，语料库和机器翻译
- 多义性和利用上下文消除歧义性





## 第2章 自然语言处理——从规则到统计

知道20世纪90年代初，才转变过来。几天，几乎不再有科学家自称是传统的基于规则方法的捍卫者。

基于统计的自然语言处理方法，在数学模型上和通信是相通的，甚至就是相同的。



## 第3章 统计语言模型

序列S在文本中出现的可能性：

$P(S)=P(w_1,w_2,...,w_n)$

利用条件概率公式展开：

$P(w_1,w_2,...,w_n)=P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_1,w_2) \cdot \cdot \cdot P(w_n|w_1,w_2,...w_{n-1})$

马尔可夫假设：

$P(S)=P(w_1) \cdot P(w_2|w_1) \cdot P(w_3|w_2) \cdot \cdot \cdot P(w_i|w_{i-1}) \cdot \cdot \cdot  P(w_n|w_{n-1})$

条件概率计算：

$$P(w_i|w_{i-1})=\frac{P(w_{i-1},w_i)}{P(w_{i-1})}$$

相对频度：

$$f(w_{i-1},w_i)= \frac{C(w_{i-1},w_i)}{Corpus}$$

$$f(w_{i-1})= \frac{C(w_{i-1})}{Corpus}$$

Corpus 表示语料库大小。

$C(w_{i-1},w_i)$ 表示 $w_{i-1},w_i$ 这对词在统计文本中前后相邻出现了多少次。

$C(w_{i-1})$ 表示 $w_{i-1}$ 本身在统计文本中出现了多少次。

根据大数定理，只要统计量猪狗，相对频度就等于概率：

$$P(w_{i-1},w_i) \approx \frac{C(w_{i-1},w_i)}{Corpus}$$

$$P(w_{i-1}) \approx  \frac{C(w_{i-1})}{Corpus}$$

故：

$$P(w_i|w_{i-1})=\frac{P(w_{i-1},w_i)}{P(w_{i-1})} \approx \frac{C(w_{i-1},w_i)}{C(w_{i-1})}$$



**高阶语言模型**

**模型的训练、零概率为和平滑方法**

古德 - 图灵估计



## 第4章 浅谈分词

中文分词



## 第5章 隐含马尔可夫模型

$$s_1,s_2,s_3, ... =\underset{all}{Arg} \underset{s_1,s_2,s_3, ...}{Max}P(s_1,s_2,s_3, ...|o_1,o_2,o_3,...)$$

Hidden Markov Model

无监督训练方法：鲍姆 - 韦尔奇算法

解码算法：维特比算法



## 第6章 信息的度量和作用

**信息熵（Entropy）**

香农

对于任意一个随机变量X，它的熵定义如下：

$$H(X)=-\sum_{x \in X}P(x)logP(x)$$

**信息的作用**

**互信息**

假定有两个随机事件X和Y，它们的互信息（Mutual Information）定义如下：

$$I(X;Y)=\sum_{x \in X, y \in Y}log \frac{P(x,y)}{P(x)P(y)}$$

$I(X;Y)=H(X)-H(X|Y)$

**相对熵**

词频率 - 逆向文档频率（TF - IDF）



## 第7章 贾里尼克和现代语言处理

贾里尼克（Frederek Jelinek）



## 第8章 简单之美——布尔代数和搜索引擎

